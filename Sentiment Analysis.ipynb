{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cfe760",
   "metadata": {},
   "source": [
    "Sentiment Analysis with Transformer-based model\n",
    "       \n",
    "       Full Analysis approach\n",
    "\n",
    "        Read reviews from SQL Server Databse,\n",
    "\n",
    "        Clean text\n",
    "\n",
    "        Run a transformer sentiment classifier (uses cardiffnlp/twitter-roberta-base-sentiment which outputs Negative/Neutral/Positive),\n",
    "\n",
    "        Evaluate against rating-derived weak labels (so you can see how model aligns with your ratings),\n",
    "\n",
    "        Write results to CSV and (optionally) back to SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd98de",
   "metadata": {},
   "source": [
    "Install Libraries:\n",
    "\n",
    "pip install pandas sqlalchemy pyodbc transformers torch tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3411a",
   "metadata": {},
   "source": [
    "\n",
    "End-to-end sentiment analysis for reviews stored in SQL Server.\n",
    "\n",
    "Model: cardiffnlp/twitter-roberta-base-sentiment (negative/neutral/positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c970b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Libraties\n",
    "\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e479db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\farhan\\AppData\\Local\\Temp\\ipykernel_5760\\2373366630.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  SQL_SERVER = \"DESKTOP-NN982MH\\SQLEXPRESS\"\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Defining Values\n",
    "# ---------------------\n",
    "\n",
    "SQL_SERVER = \"DESKTOP-NN982MH\\SQLEXPRESS\"         \n",
    "SQL_DB     = \"PortfolioProject_MarketingAnalytics\"           \n",
    "SQL_UID    = \"\"                        \n",
    "SQL_PWD    = \"\"                       \n",
    "USE_TRUSTED_CONNECTION = True          # True -> windows auth (trusted); False -> use UID/PWD\n",
    "\n",
    "TABLE_NAME = \"dbo.customer_reviews\"    \n",
    "OUTPUT_CSV = \"customer_reviews_with_sentiment.csv\"\n",
    "BATCH_SIZE = 32                        \n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0098dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# SQL connection\n",
    "# ---------------------\n",
    "\n",
    "def get_sqlalchemy_engine():\n",
    "    if USE_TRUSTED_CONNECTION:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={SQL_SERVER};DATABASE={SQL_DB};Trusted_Connection=yes;\"\n",
    "        )\n",
    "    else:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={SQL_SERVER};DATABASE={SQL_DB};UID={SQL_UID};PWD={SQL_PWD};\"\n",
    "        )\n",
    "    params = urllib.parse.quote_plus(conn_str)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\", fast_executemany=True)\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9be6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from SQL...\n",
      "Loaded 1363 rows.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 1) Load data from SQL\n",
    "# ---------------------\n",
    "engine = get_sqlalchemy_engine()\n",
    "sql = f\"SELECT CustomerID, ProductID, ReviewDate, Rating, ReviewText FROM {TABLE_NAME}\"\n",
    "print(\"Loading data from SQL...\")\n",
    "df = pd.read_sql(sql, engine)\n",
    "print(f\"Loaded {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f2e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping empty reviews: 1363 rows.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 2) Basic text cleaning\n",
    "# ---------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    # remove URLs\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
    "    # remove HTML tags\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)\n",
    "    # replace multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # optional: remove excessive punctuation\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return s.strip()\n",
    "\n",
    "df['ReviewText'] = df['ReviewText'].fillna(\"\").astype(str)\n",
    "df['clean_text'] = df['ReviewText'].apply(clean_text)\n",
    "\n",
    "# drop rows with empty text (optional)\n",
    "df = df[df['clean_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"After dropping empty reviews: {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5541b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\farhan\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 3) Prepare model pipeline\n",
    "# ---------------------\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create HF pipeline; set device=0 for GPU, -1 for CPU\n",
    "device = 0 if (os.environ.get(\"CUDA_VISIBLE_DEVICES\") or False) else -1\n",
    "# If you have GPU but environment variable isn't set, set device=0 manually above.\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# label mapping fallback (some models return LABEL_0 style labels)\n",
    "_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "def interpret_label(label_str: str):\n",
    "    lbl = label_str.lower()\n",
    "    if lbl in (\"positive\", \"negative\", \"neutral\"):\n",
    "        return lbl.capitalize()\n",
    "    # coerce LABEL_n\n",
    "    if lbl.startswith(\"label_\"):\n",
    "        idx = int(lbl.split(\"_\")[1])\n",
    "        return _map.get(idx, \"neutral\").capitalize()\n",
    "    # fallback\n",
    "    return lbl.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaf0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:47<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 4) Batch inference\n",
    "# ---------------------\n",
    "\n",
    "texts = df['clean_text'].tolist()\n",
    "pred_labels = []\n",
    "pred_scores = []\n",
    "\n",
    "print(\"Running inference in batches...\")\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE)):\n",
    "    batch = texts[i:i+BATCH_SIZE]\n",
    "    # pipeline will return list of dicts, each with 'label' and 'score'\n",
    "    results = sentiment_pipe(batch, truncation=True, max_length=512)\n",
    "    for res in results:\n",
    "        label = interpret_label(res.get('label', \"neutral\"))\n",
    "        score = float(res.get('score', 0.0))\n",
    "        pred_labels.append(label)\n",
    "        pred_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8739158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to dataframe\n",
    "\n",
    "df['Sentiment_Pred'] = pred_labels\n",
    "df['Sentiment_Score'] = pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9c0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report (model vs rating-derived labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6302    0.9142    0.7461       233\n",
      "     Neutral     0.8312    0.2207    0.3488       290\n",
      "    Positive     0.8861    1.0000    0.9396       840\n",
      "\n",
      "    accuracy                         0.8195      1363\n",
      "   macro avg     0.7825    0.7116    0.6781      1363\n",
      "weighted avg     0.8306    0.8195    0.7808      1363\n",
      "\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "          Pred_Pos  Pred_Neu  Pred_Neg\n",
      "True_Pos       840         0         0\n",
      "True_Neu       101        64       125\n",
      "True_Neg         7        13       213\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 5) Quick evaluation vs rating (weak labels)\n",
    "# ---------------------\n",
    "def rating_to_sentiment(r):\n",
    "    try:\n",
    "        r = float(r)\n",
    "    except:\n",
    "        return \"Neutral\"\n",
    "    if r >= 4.0:\n",
    "        return \"Positive\"\n",
    "    elif r == 3.0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "df['Rating_Label'] = df['Rating'].apply(rating_to_sentiment)\n",
    "\n",
    "print(\"\\nClassification report (model vs rating-derived labels):\")\n",
    "print(classification_report(df['Rating_Label'], df['Sentiment_Pred'], digits=4))\n",
    "\n",
    "cm = confusion_matrix(df['Rating_Label'], df['Sentiment_Pred'], labels=[\"Positive\",\"Neutral\",\"Negative\"])\n",
    "cm_df = pd.DataFrame(cm, index=[\"True_Pos\",\"True_Neu\",\"True_Neg\"], columns=[\"Pred_Pos\",\"Pred_Neu\",\"Pred_Neg\"])\n",
    "print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0f519",
   "metadata": {},
   "source": [
    "1. Overall accuracy\n",
    "\n",
    "        accuracy = 0.8195  (~82%)\n",
    "\n",
    "        The model agrees with the rating-derived sentiment labels 82% of the time. That’s strong overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8448c7",
   "metadata": {},
   "source": [
    "2. Class-wise metrics\n",
    "\n",
    "        🔹 Positive\n",
    "\n",
    "                Precision: 0.8861 → 89% of predicted positives are correct.\n",
    "\n",
    "                Recall: 1.0000 → Model caught all true positives.\n",
    "\n",
    "                F1: 0.9396 → Excellent.\n",
    "                ✅ Interpretation: The model is highly reliable at detecting Positive reviews.\n",
    "\n",
    "\n",
    "        🔹 Negative\n",
    "\n",
    "                Precision: 0.6302 → 63% of predicted negatives are correct (so ~37% false positives).\n",
    "\n",
    "                Recall: 0.9142 → The model catches most negative reviews, but sometimes mislabels other reviews as negative.\n",
    "\n",
    "                F1: 0.7461 → Decent but not perfect.\n",
    "                ⚠️ Interpretation: The model tends to over-predict negatives.\n",
    "\n",
    "        🔹 Neutral\n",
    "\n",
    "                Precision: 0.8312 → When it predicts Neutral, it’s right most of the time.\n",
    "\n",
    "                Recall: 0.2207 → But it misses most actual neutral reviews (only 22% detected).\n",
    "\n",
    "                F1: 0.3488 → Weak overall.\n",
    "                ⚠️ Interpretation: The model struggles to identify Neutral sentiment (likely because ratings → sentiment mapping is noisy, and human-written neutral reviews are often subtle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b0a46",
   "metadata": {},
   "source": [
    "3. Confusion Matrix\n",
    "\n",
    "              Pred_Pos  Pred_Neu  Pred_Neg\n",
    "True_Pos         840        0        0\n",
    "True_Neu         101       64      125\n",
    "True_Neg           7       13      213\n",
    "\n",
    "\n",
    "        True_Pos (840) → All 840 actual positive reviews were predicted as Positive (perfect recall).\n",
    "\n",
    "        True_Neg (233) → Most negatives correctly predicted (213), but a few got confused as Neutral (13) or Positive (7).\n",
    "\n",
    "        True_Neu (290) → Huge problem:\n",
    "\n",
    "        101 mislabeled as Positive,\n",
    "\n",
    "        125 mislabeled as Negative,\n",
    "\n",
    "        Only 64 correctly predicted Neutral.\n",
    "\n",
    "👉 Takeaway: The model basically collapses Neutral into Positive/Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606a2850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to customer_reviews_with_sentiment.csv\n",
      "Saved classification_report.csv\n",
      "Saved confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 6) Save outputs\n",
    "# ---------------------\n",
    "\n",
    "df_out = df[['CustomerID','ProductID','ReviewDate','Rating','ReviewText',\n",
    "             'clean_text','Rating_Label','Sentiment_Pred','Sentiment_Score']]\n",
    "\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\nSaved predictions to {OUTPUT_CSV}\")\n",
    "\n",
    "# Save classification report to CSV\n",
    "report_dict = classification_report(df['Rating_Label'], df['Sentiment_Pred'], \n",
    "                                    digits=4, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df.to_csv(\"classification_report.csv\", index=True)\n",
    "print(\"Saved classification_report.csv\")\n",
    "\n",
    "# Save confusion matrix to CSV\n",
    "cm = confusion_matrix(df['Rating_Label'], df['Sentiment_Pred'], \n",
    "                      labels=[\"Positive\",\"Neutral\",\"Negative\"])\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[\"True_Pos\",\"True_Neu\",\"True_Neg\"], \n",
    "                     columns=[\"Pred_Pos\",\"Pred_Neu\",\"Pred_Neg\"])\n",
    "cm_df.to_csv(\"confusion_matrix.csv\", index=True)\n",
    "print(\"Saved confusion_matrix.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
